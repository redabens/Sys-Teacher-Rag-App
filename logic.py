from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_core.callbacks import BaseCallbackHandler
import uvicorn
import os
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware



load_dotenv()

os.environ["LANGCHAIN_API_KEY"]= os.getenv("LANGCHAIN_API_KEY")
os.environ["LANGCHAIN_TRACING_V2"]= "true"

# Constantes
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200
K_DOCUMENTS = 10
EMBEDDING_MODEL_NAME = "nomic-embed-text"
PDF_DIR = "./Sys"


# Initialiser le LLM (Groq Api Inference)
llm = ChatOllama(model="llama3")
# Fonctions
# Fonction pour traiter les documents
def process_documents(PDF_DIR=PDF_DIR):
    docs = PyPDFDirectoryLoader(PDF_DIR).load()
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", ".", "?", "!", " ", ""],
    )
    documents = text_splitter.split_documents(docs)
    return docs, documents

# Fonction pour g√©n√©rer un r√©sum√© des documents
def get_rag_summary(docs):
    try:
        # Cr√©er un prompt pour g√©n√©rer un r√©sum√©
        summary_prompt = ChatPromptTemplate.from_template("""
        Tu es un expert en syst√®mes d'exploitation charg√© de cr√©er un r√©sum√© concis et structur√©.
        
        Analyse le contexte suivant :
        <context>
        {context}
        </context>
        
        G√©n√®re moi un sommaire detaillers de tous les points sujets couverts dans le contexte.
        Tu dois detailler chaque point bri√©vement et expliquer son importance.
        
        Ce sommaire sera utilis√© comme contexte a un llm pour am√©liorer les questions de l'utilisateur.
        """)
        
        # Concat√©ner tous les page_content
        full_context = "\n\n".join([doc.page_content for doc in docs])
        print(f"Contexte complet g√©n√©r√©, longueur: {len(full_context)}")
        
        # Cr√©er une cha√Æne simple pour le r√©sum√©
        summary_chain = summary_prompt | llm
        
        # Obtenir le r√©sum√© en passant directement le contexte complet
        response = summary_chain.invoke({"context": full_context})
        print(f"R√©ponse du LLM: {response}")
        
        return response
    except Exception as e:
        print(f"Erreur dans get_rag_summary: {str(e)}")
        raise e

# Cr√©er les embeddings avec Ollama Embeddings
embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)
docs, documents = process_documents()
vectorstore = Chroma.from_documents(documents, embedding=embeddings)


# Prompts

# Template du prompt principal
main_prompt = ChatPromptTemplate.from_template("""
    Vous √™tes un **professeur expert**, charg√© d'expliquer un sujet de mani√®re **p√©dagogique, compl√®te et pr√©cise**.  
    Votre mission est d'enseigner le sujet **comme si vous expliquiez √† des √©tudiants**, en les aidant √† comprendre en profondeur.  

    ### **M√©thodologie d'analyse et d'explication :**  
    üîç **Analyse du contexte fourni** :  
    - Lisez et comprenez attentivement le contexte avant de formuler une r√©ponse.  
    - Identifiez les concepts cl√©s et les informations pertinentes li√©es √† la question pos√©e.  
    - V√©rifiez si certaines informations sont manquantes ou si des erreurs doivent √™tre corrig√©es.  

    üìö **M√©thodologie d'enseignement** :  
    1Ô∏è‚É£ **Introduction claire** : Pr√©sentez le sujet en expliquant son importance et son utilit√©.  
    2Ô∏è‚É£ **Explication progressive** : D√©veloppez les concepts de mani√®re claire, en partant des bases avant d'aller vers des notions plus avanc√©es.  
    3Ô∏è‚É£ **Illustrations et analogies** : Utilisez des exemples concrets et des analogies pour faciliter la compr√©hension.  
    4Ô∏è‚É£ **Correction des confusions fr√©quentes** : Anticipez les erreurs ou malentendus que les apprenants pourraient avoir.  
    5Ô∏è‚É£ **Exemples d'application** : Montrez comment ces connaissances peuvent √™tre appliqu√©es en pratique.  
    6Ô∏è‚É£ **Regroupement final** : Pr√©sentez **tout ce qui a √©t√© expliqu√©** sous une forme claire et organis√©e.  
    7Ô∏è‚É£ **Ajoutez une section "üìå Ce que les √©tudiants oublient souvent"**

    ### **Historique de la conversation :**
    {chat_history}
    
    ### **Contexte donn√© :**  
    <context>  
    {context}  
    </context>  

    ### **Question actuelle :**  
    {input}  

    üìñ **Explication p√©dagogique d√©taill√©e :**
    """)

# Cr√©ation des cha√Ænes
retriever = vectorstore.as_retriever(search_kwargs={"k": K_DOCUMENTS})
document_chain = create_stuff_documents_chain(llm=llm, prompt=main_prompt)
qa_chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=document_chain)

app = FastAPI(
    title="Langchain Server",
    version="1.0",
    description="A server that uses Langchain for a teacher chatbot",
)
# Set all CORS enabled origins
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
)

@app.get("/get_rag_summary")
async def get_summary_endpoint():
    try:
        summary = get_rag_summary(docs)
        print(summary)
        return {"summary": summary.content}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
@app.post("/chat")
async def chat(request: Request):
    try:
        data = await request.json()
        input = data.get("input")
        chat_history = data.get("chat_history")
        response = qa_chain.invoke({"input": input, "chat_history": chat_history})
        return {"answer": response["answer"]}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


        
if __name__ == "__main__":
    uvicorn.run(app, host="localhost", port=8000, log_level="info")
